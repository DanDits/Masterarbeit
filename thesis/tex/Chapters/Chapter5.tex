% Chapter 5

\chapter{Stochastische Galerkin-Methode}
Die stochastische Klein-Gordon-Gleichung war für eine $N$-dimensionale Zufallsvariable $Y$ und $y\in\R^N$ definiert durch
\begin{align}
\label{eqn:galerkin_skgg}
\dtt{u}(t,x,y)&=\alpha(y) \Laplace u(t,x,y) - \beta(x,\omega)u(t,x,y), \: t>0, \, x\in \Torus^d\\
u(0,x,y)&=g(x,y), \: x\in \Torus^d\nonumber\\
\dt{u}(0,x,y)&=h(x,y), \: x\in \Torus^d\nonumber
\end{align}
Seien $\Phi_m(Y)$ die zu $Y$ gehörenden gPC Basisfunktionen. Diese erfüllen die Orthogonalitätsbedingung 
\[\E[\Phi_i(Y)\Phi_j(Y)]=\delta_{ij}\] 
Sei $\Poly_P^N$ der Raum aller Polynome mit summiertem Grad kleiner gleich $P$ und Dimension $\binom{N+P}{P}$. Die gPC Projektion der Lösung der sKGG zu einem fixen Zeitpunkt $T>0$ und Ort $x\in\Torus^d$ ist definiert durch
\begin{align*}
u_M(T,x,Y)&=\sum_{m=0}^M\hat{u}_m(T,x)\Phi_m(Y)\in\Poly_P^N,\quad  M+1=\binom{N+P}{P}\\
\hat{u}_m(T,x)&=\E[u(T,x,Y)\Phi_m(Y)]
\end{align*}
Ist $\rho$ die Dichtefunktion von $Y$, so stimmt die gPC Projektion mit der Bestapproximation in $L_\rho^2$ überein. Diese Approximation ist im $L_\rho^2$ Sinne optimal, allerdings benötigen wir zur Berechnung der Koeffizienten $\hat{u}_m$ die unbekannte exakte Lösung $u(T,x,Y)$.\\
Der stochastische Galerkin Ansatz ist eine Erweiterung des klassischen Galerkin Ansatzes. Sei dazu eine weitere Approximation $v_M\in\Poly_P^N$ gegeben durch
\begin{equation}
v_M(t,x,Y)=\sum_{m=0}^M\hat{v}_m(t,x)\Phi_m(Y)
\end{equation}
Die Forderung an $v_M$ ist dann, dass das Residuum der sKGG (\ref{eqn:galerkin_skgg}) orthogonal zu allen Polynomen aus $\Poly_P^N$ ist. Eine Motivation dafür ist die schwache Lösung der sKGG, welche diese Orthogonalität für alle Polynome $\Poly^N$ erfüllt. Ist die schwache Lösung hinreichend regulär, so stimmt sie mit der Lösung der sKGG überein.
Konkret lautet die Orthogonalitätsbedingung an das Residuum für $t>0$ und $x\in\Torus^d$
\begin{equation}
\label{eqn:galerkinOrtho}
\E\left[\left(\dtt{v_M}(t,x,Y)-\alpha(Y)\Laplace v_M(t,x,Y)+\beta(x,Y)v_M(t,x,Y)\right)\Phi_k\right]=0
\end{equation} 
für alle $k=0,\dots,M$.\\
Wir wollen jetzt aus der Orthogonalitätsbedingung (\ref{eqn:galerkinOrtho}) eine Gleichung für die Koeffizienten $\hat{v}_m$ gewinnen. Dazu setzen wir die Definition von $v_M$ in die Gleichung ein und erhalten
\begin{align*}
\sum_{m=0}^M\dtt{\hat{v}}_m(t,x)\underbrace{\E[\Phi_m(Y)\Phi_k(Y)]}_{=\delta_{mk}}&=\E\left[\alpha(Y)\sum_{m=0}^M\Laplace \hat{v}_m(t,x)\Phi_m(Y)\Phi_k(Y)\right]\\
&\quad -\E\left[\beta(x,Y)\sum_{m=0}^M\hat{v}_m(t,x)\Phi_m(Y)\Phi_k(Y)\right]\\
\equivalent \qquad \dtt{\hat{v}}_k(t,x)&=\sum_{m=0}^M\Laplace \hat{v}_m(t,x)\underbrace{\E[\alpha(Y)\Phi_m(Y)\Phi_k(Y)]}_{=a_{mk}}\\
&\quad -\sum_{m=0}^M\hat{v}_m(t,x)\underbrace{\E[\beta(x,Y)\Phi_m(Y)\Phi_k(Y)]}_{=b_{mk}(x)}\\
\equivalent \qquad \dtt{\hat{v}}_k(t,x)&=a_k\Laplace \hat{v}(t,x)-b_k(x)\hat{v}(t,x)\\
\intertext{mit $ a_k=(a_{0k},\dots,a_{Mk}),\quad b_k(x)=(b_{0k}(x),\dots,b_{Mk}(x)),\quad \hat{v}(t,x)=(\hat{v}_0(t,x),\dots,\hat{v}_M(t,x))^T$}\\
\equivalent \qquad \dtt{\hat{v}}(t,x)&=A\Laplace \hat{v}(t,x)-B(x)\hat{v}(t,x)
\end{align*}
Die Matrizen $A$ und $B(x)$ werden durch die Zeilenvektoren $a_k$ und $b_k(x)$ gebildet und sind symmetrisch. Die benötigten Koeffizienten der Approximation $v_M$ sind gekoppelt und als Lösung folgender partieller Differentialgleichung zu erhalten
\begin{equation}
\label{eqn:galerkin_dgl}
\begin{split}
\dtt{\hat{v}}(t,x)&=A\Laplace \hat{v}(t,x)-B(x)\hat{v}(t,x)\\
\hat{v}(0,x)&=\hat{g}(x)\\
\dt{\hat{v}}(0,x)&=\hat{h}(x)\\
\end{split}
\end{equation}
Dabei gilt für die Anfangswerte $\hat{v}(0,x)$ und $\dt{\hat{v}}(0,x)$ wegen 
\[v_M(0,x,Y)=\sum_{m=0}^M\hat{v}_m(0,x)\Phi_m(Y)\]
und
\begin{align*}
g(x,y)&\approx v_M(0,x,y)\\
h(x,y)&\approx \dt{v}_M(0,x,y)
\end{align*}
die Darstellung mithilfe der Koeffizienten der Bestapproximation von $g$ und $h$
\begin{align*}
\hat{v}_m(0,x)=\hat{g}_m=\E[g(x,Y)\Phi_m(Y)]\\
\dt{\hat{v}}_m(0,x)=\hat{h}_m=\E[h(x,Y)\Phi_m(Y)]
\end{align*}
\section{Splitting für den Galerkin-Ansatz}
Die partielle Differentialgleichung (\ref{eqn:galerkin_dgl}) ist ein System mit Zeitableitung zweiter Ordnung und vektorwertig mit Größe $M+1$. Um eine numerische Lösung des Systems zu erhalten, bietet sich wieder ein Splitting-Ansatz an. Zuerst wollen wir das System aber so weit wie möglich entkoppeln.
\begin{maththeorem}
\label{th:galerkin_posdef}
Ist $\alpha(y)>c$ für ein $c>0$, so ist die Matrix $A$ aus dem Galerkin-Ansatz positiv definit und besitzt nur positive Eigenwerte $\lambda>c$.\\
Ist $\norm{\alpha}_\infty < \infty$, so gilt $\lambda\le \norm{\alpha}_\infty$.
\end{maththeorem}
\begin{proof}
Sei $z\in\R^{M+1}\setminus \lbrace 0\rbrace$ beliebig. Dann gilt wegen $a_{ij}=\int \alpha(y)\Phi_i(y)\Phi_j(y)\rho(y)dy$
\[(Az)_i=\int \alpha(y)\Phi_i(y)\underbrace{\left(\sum_{j=0}^M\Phi_j(y)z_j\right)}_{=\tilde{z}(y)}\rho(y)dy\]
und daher
\begin{align*}
z^TAz&=\int \alpha(y)\left(\sum_{i=0}^M\Phi_i(y)z_i\right)\tilde{z}(y)\rho(y)dy\\
&=\int\underbrace{\alpha(y)}_{>c}\underbrace{\tilde{z}^2(y)}_{>0 \text{ f.ü.}}\underbrace{\rho(y)}_{>0}dy\\
&>c\int \tilde{z}^2(y)\rho(y)dy\\
&=c\sum_{i=0}^Mz_i\sum_{j=0}^Mz_j\underbrace{\int \Phi_i(y)\Phi_j(y)\rho(y)dy}_{=\delta_{ij}}\\
&=cz^Tz
\end{align*}
Die Abschätzung nach oben ergibt sich ähnlich, da für einen Eigenvektor $z\neq 0$ zum Eigenwert $\lambda$ gilt
\[|\lambda| z^Tz=|z^TAz|=\left|\int \alpha(y)\tilde{z}^2(y)\rho(y)dy\right|\leq \norm{\alpha}_\infty z^Tz\]
\end{proof}
Unter entsprechenden Voraussetzungen an $B(x)$ gelten die Aussagen von Satz \ref{th:galerkin_posdef} punktweise auch für $B(x)$.
\subsection{Entkopplung des ersten Teils}
Wir können $A\in\R^{M+1\times M+1}$ zerlegen in
\[A=S_1D_1S_1^T,\quad \text{$D_1$ diagonal und $S_1$ orthogonal}\]
Wir definieren dann 
\[w(t,x)\coloneqq S_1^T\hat{v}(t,x),\quad t>0,x\in\Torus^d\]
und schreiben das System (\ref{eqn:galerkin_dgl}) in der Form
\begin{equation}
\label{eqn:galerkin_dgl_transformed}
\begin{split}
\dtt{w}(t,x)&=D_1\Laplace w(t,x)-S_1^TB(x)S_1w(t,x)\\
w(0,x)&=S_1^T\hat{g}(x)\\
\dt{w}(0,x)&=S_1^T\hat{h}(x)\\
\end{split}
\end{equation}
Schreiben wir es anschließend formal um in ein System mit Zeitableitung erster Ordnung, so gilt für $\tilde{w}\coloneqq \dt w$ und ein Gewicht $\kappa\in[0,1]$
\[\dt\begin{pmatrix}w\\ \tilde{w}\end{pmatrix}(t,x)=\left[\begin{pmatrix}0 & \kappa I_{M+1}\\ D_1\Laplace& 0\end{pmatrix}+\begin{pmatrix}0&(1-\kappa)I_{M+1}\\-S_1^TB(x)S_1 & 0\end{pmatrix}\right]
\begin{pmatrix}w\\\tilde{w}\end{pmatrix}(t,x)\]
Das Splitting besteht dann aus den zwei Gleichungen
\begin{equation}
\label{eqn:galerkin_split1}
\dt\begin{pmatrix}w_1\\ \tilde{w}_1\end{pmatrix}(t,x)=\begin{pmatrix}0 & \kappa I_{M+1}\\ D_1\Laplace& 0\end{pmatrix}\begin{pmatrix}w_1\\\tilde{w}_1\end{pmatrix}(t,x)
\end{equation}
und
\begin{equation}
\label{eqn:galerkin_split2}
\dt\begin{pmatrix}w_2\\ \tilde{w}_2\end{pmatrix}(t,x)=\begin{pmatrix}0&(1-\kappa)I_{M+1}\\-S_1^TB(x)S_1 & 0\end{pmatrix}\begin{pmatrix}w_2\\\tilde{w}_2\end{pmatrix}(t,x)
\end{equation}
Diese sind wie schon in Kapitel \ref{chapter:solver_kgg} mithilfe der Matrixexponentialfunktion lösbar. Beachte, dass die Gleichung (\ref{eqn:galerkin_split1}) nach Umsortierung in $2\times 2$ Systeme entkoppelt ist, da $D_1$ diagonal ist. Die Matrixexponentialfunktion der entkoppelten $2\times 2$ Systeme ist in Algorithmus \ref{alg:kgg_solver} als \emph{First\_Part\_Solver} in einfacher Darstellung direkt gegeben. Der erste Teil des Splittings besteht somit aus dem Lösen von $M+1$ unabhängigen "'Wellengleichungen"' (Name passend für $\kappa=1$). Die Eigenwerte $d_i$ von $A$ sind die Quadrate der Wellengeschwindigkeiten. Abschätzungen für diese, insbesondere Positivität, erhalten wir mithilfe von Satz \ref{th:galerkin_posdef} unter entsprechenden Voraussetzungen an $\alpha(y)$.

\subsection{Entkopplung des zweiten Teils}
Analog zum ersten Teil können wir für jedes fixe $x\in\Torus^d$ die symmetrische Matrix $S_1^TB(x)S_1\in\R^{M+1\times M+1}$ zerlegen in $S_1^TB(x)S_1=S_2(x)D_2(x)S_2^T(x)$ mit $S_2(x)$ orthonormal und $D_2(x)$ diagonal.\\
Definiere
\[z(t,x)\coloneqq S_2^T(x) w_2(t,x), \quad \tilde{z}(t,x)=\dt{z}(t,x)\]
Dann ist (\ref{eqn:galerkin_split2}) äquivalent zum System
\begin{equation}
\label{eqn:galerkin_split2_decoup}
\dt\begin{pmatrix}z\\ \tilde{z}\end{pmatrix}(t,x)=\begin{pmatrix}0&(1-\kappa)I_{M+1}\\-D_2(x) & 0\end{pmatrix}\begin{pmatrix}z\\\tilde{z}\end{pmatrix}(t,x)
\end{equation}
Das System (\ref{eqn:galerkin_split2_decoup}) ist nach Umsortierung in $2\times 2$ Blöcke entkoppelt. Die Matrixexponentialfunktion dieser Blöcke können wir mit der Funktion \emph{Second\_Part\_Solver} aus Algorithmus \ref{alg:kgg_solver} effizient lösen. Der große Vorteil hierbei ist, dass die Berechnung der Matrixexponentialfunktion der $2M+2\times 2M+2$ Matrix aus (\ref{eqn:galerkin_split2}) für jedes $x$ im Gitter entfällt. Der Trade-off ist, dass in jedem Splittingschritt die Transformation und Rücktransformation mit $S_2(x)$ durchgeführt werden muss. Dieser zusätzliche Aufwand fällt jedoch im Gegensatz zur teuren Matrixexponentialfunktion, welche die Blockstruktur der Matrix nicht berücksichtigt, nicht stark ins Gewicht.
\subsection{Algorithmus}
In Algorithmus \ref{alg:galerkin} ist die Vorgehensweise für den Galerkin-Ansatz zum Lösen der stochastischen KGG skizziert. Als Basis dienen dabei die Kernfunktionen des allgemeinen KGG Lösers aus Algorithmus \ref{alg:kgg_solver}, welche in einem schnellen Strang-Splitting eingebettet sind.
\begin{algorithm}[ht]
    \caption{Galerkin für die stochastische KGG.}
    \label{alg:galerkin}
    \begin{algorithmic}[1] % The number tells where the line numbering should start
        \Function{Galerkin}{$H, u_0, v_0, \alpha,\beta,\kappa,Y, M,\tau, T$} 
            	\State $\Phi_m\gets$ $m$-te orthonormale gPC-Basis-Funktion zu $Y$, $\quad m=0,\dots,M$
              \State $A_{mk}\gets \E[\alpha(Y)\Phi_m(Y)\Phi_k(Y)],\quad m,k=0,\dots,M$
              \State $A=S_1D_1S_1^T$, $S_1$ orthonormal, $D_1$ diagonal
              \State $x_H\gets$ Ortsdiskretisierung der Größe $H$
              \For{$x\in x_H$}
              	\State $B_{mk}(x)\gets \E[\beta(x,Y)\Phi_m(Y)\Phi_k(Y)],\quad m,k=0,\dots,M$
              	\State $S_1^TB(x)S_1=S_2(x)D_2(x)S_2^T(x)$, $S_2(x)$ orthonormal, $D_2(x)$ diagonal
              \EndFor
              \State
              \LineComment{FastStrang Splitting für entkoppelte Systeme:}
              \State $g(x)\gets S_1^T\cdot (\E[u_0(x,Y)\Phi_m(Y)])_{m=0,\dots,M}$
              \State $h(x)\gets S_1^T\cdot (\E[v_0(x,Y)\Phi_m(Y)])_{m=0,\dots,M}$
              \State $g,h\gets \text{Multi\_First\_Part\_Solver}(H, g, h, \diag(D_1), \kappa, \frac{\tau}{2})$
            \State $g,h\gets \text{Multi\_Second\_Part\_Solver}(x_H, g, h, \diag(D_2), S_2, 1-\kappa, \tau)$
            \For{$i=1,\dots,\frac{T}{\tau}-1$} \Comment{Annahme: $\frac{T}{\tau}\in\N$} 
            	\State $g,h\gets \text{Multi\_First\_Part\_Solver}(H, g, h, \diag(D_1), \kappa, \tau)$
            \State $g,h\gets \text{Multi\_Second\_Part\_Solver}(x_H, g, h, \diag(D_2), S_2, 1-\kappa, \tau)$
            \EndFor
            \State $g,h\gets \text{Multi\_First\_Part\_Solver}(H, g, h, \diag(D_1), \kappa, \frac{\tau}{2})$
            \State
            \LineComment{Rücktransformation und Extraktion der Statistiken:}
            \State $\hat{v}\gets S_1g$  
            \State $\mu\gets \hat{v}_{0,j=1,\dots,H}$ 
			\State $\sigma^2_j\gets \sum_{m=1,\dots,M}\hat{v}^2_{mj}$
			\State \textbf{return} $\mu,\sigma^2$
        \EndFunction
        \Function{Multi\_First\_Part\_Solver}{$H, g, h, \alpha, \kappa, \tau$}
        	\For{$i=0,\dots,M$}
        		\State $\tilde{g}_i,\tilde{h}_i\gets \text{First\_Part\_Solver}(H,g_i, h_i,\alpha_i,\kappa,\tau)$
        	\EndFor
        	\State \textbf{return} $(\tilde{g}_0,\dots,\tilde{g}_M)^T, (\tilde{h}_0,\dots,\tilde{h}_M)^T$
        \EndFunction
        \Function{Multi\_Second\_Part\_Solver}{$x_H,g,h,\beta,S_2,\tilde{\kappa},\tau$}
        	\State $g,h\gets S_2^T(x_H)g, S_2^T(x_H)h$
        	\For{$i=0,\dots,M$}
        		\State $\tilde{g}_i,\tilde{h}_i\gets \text{Second\_Part\_Solver}(x_H,g_i,h_i,\beta_i,\tilde{\kappa},\tau)$
        	\EndFor
        	\State \textbf{return} $S_2(x_H)\cdot (\tilde{g}_0,\dots,\tilde{g}_M)^T, S_2(x_H)\cdot (\tilde{h}_0,\dots,\tilde{h}_M)^T$
        \EndFunction
        
    \end{algorithmic}
\end{algorithm}
\section{Numerische Ergebnisse}
\todo[inline]{Numerische Ergebnisse (über Stopzeit, pro Knotenpunkt,...}
\begin{maththeorem}[Adaption von Theorem 2.2 aus \autocite{davidgottliebdongbinxiu2008}]

\end{maththeorem}
\todo[inline]{Adaption von Beweis von Theorem 2.2}
\todo[inline]{Vergleiche mehrdim, unstetig, nicht diffbar mit Collocation}