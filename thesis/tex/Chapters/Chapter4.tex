% Chapter 4

\chapter{Stochastische Collocation}
Wie bereits die Monte-Carlo-Methode basieren auch stochastische Collocationsverfahren auf \emph{sampling}, d.h. der wiederholten Auswertung von vielen Zufallsexperimenten. Bei der Monte-Carlo-Methode werden die Punkte dabei abhängig von der gegebenen Verteilung unabhängig und zufällig gewählt. Die Konvergenz ist in einem stochastischen Sinne durch das Gesetz der großen Zahlen gegeben.\\
Dies ist ein großer Unterschied zu den hier vorgestellten stochastischen Collocationsverfahren. Für diese werden die Collocationspunkte nicht zufällig, sondern so gewählt, dass mithilfe von ihnen eine Polynominterpolation oder Quadratur möglich ist. Konvergenz ist dann abhängig davon, wie gut sich die approximierte stochastische Größe durch Polynome von Zufallsvariablen approximieren lässt.\\
Die beiden Ansätze \emph{Interpolation} und \emph{diskrete Projektion} (auch \emph{pseudospektraler Ansatz} genannt) werden für die stochastische KGG im Folgenden detailliert vorgestellt und miteinander verglichen.
\section*{Collocation für die stochastische KGG}
Die stochastische KGG (\ref{skgg}) für $d$ räumliche Dimensionen, $N$ stochastische Dimensionen und $y\in\R^N$ war definiert durch
\begin{align*}
\dtt{u}(t,x,y)&=\alpha(y) \Laplace_x u(t,x,y) - \beta(x,\omega)u(t,x,y), \: t>0, \, x\in \Torus^d\\
u(0,x,y)&=u_0(x,y), \: x\in \Torus^d\\
\dt{u}(0,x,y)&=v_0(x,y), \: x\in \Torus^d
\end{align*}
Im klassischen Sinne bedeutet Collocation, dass eine Approximation für eine diskrete Knotenmenge $\Theta_Y=\lbrace y_i\in\R^N\mid i=1,\dots,S\rbrace$ exakt ist. Auf die stochastische KGG übertragen, muss also für $y=y_i\in\Theta_Y$ die Lösung $u(t,x,y_i)$ für $x\in\Torus^d$ und $t>0$ vorliegen und die Approximation $w$ erfüllt $w(t,x,y_i)=u(t,x,y_i)$ für $i=1,\dots,S$. Da wir die exakte Lösung $u(t,x,y_i)$ nicht kennen, sondern für einen Zeitpunkt $T>0$ und eine Knotenmenge $\lbrace x_j\in\Torus^d\mid j=1,\dots,H\rbrace$ nur mithilfe des Strang-Splittings approximieren können, gilt also für festes $x_j$ die Collocationsbedingung lediglich approximativ:
\[w(T,x_j,y_i)=u(T,x_j,y_i)\approx \tilde{u}(T,x_j,y_i),\quad i=1,\dots,S\]
Wir wählen daher $\tau$ so klein, dass dieser Fehler im Vergleich zum eigentlichen Collocationsfehler vernachlässigbar gering ist. Zur Vereinfachung der Notation nehmen wir im Folgenden an, die Lösung $u(T,x_j,y_i)$ liege exakt vor.

\section{Interpolation}
Sei $Y$ ein $N$-dimensionaler Zufallsvektor mit stochastisch unabhängigen Komponenten und $\Theta_Y\subset I_Y$ eine zu den Verteilungen von $Y_i$ passende $S$ elementige Knotenmenge. Seien weiter $\lbrace \Phi_m \mid |m|\le P \rbrace$ die zu den Verteilungen passenden gPC Basisfunktionen und im Folgenden mit $m=0,\dots M=\binom{N+P}{P}-1$ durchnummeriert. Eine Diskussion zur Nummerierung findet man im vorherigen Kapitel.\\
Dann wählen wir als Interpolationspolynom die Funktion
\begin{equation}
\label{eqn:coll_interpol_basic}
w_M^{(j)}(Y)=\sum_{m=0}^M\hat{w}_m^{(j)}\Phi_m(Y)\in\Poly_P^N(Y),\quad j=1,\dots,H
\end{equation}
und fordern als Interpolationsbedingung
\[w_M^{(j)}(y_i)=u(T,x_j,y_i),\quad i=1,\dots,S, j=1,\dots,H\]
Die Notation macht klar, dass wir zu einem gegebenen Zeitpunkt $T>0$ und jedem festen Knotenpunkt $x_j$ die stochastische Funktion $u(T,x_j,Y)$ durch ein Polynom in $Y$ approximieren wollen.\\
Betrachten wir die Gleichung (\ref{eqn:coll_interpol_basic}) in den Knotenpunkten $y_i$ für jedes $i=1,\dots,S$, so können wir das entstehende System kompakt schreiben als
\begin{equation}
\label{eqn:interpol_compact}
A\hat{w}=u
\end{equation}
mit 
\begin{align*}
A&=(\Phi_m(y_i))_{i=1,\dots,S;m=0,\dots,M}\in\R^{S\times M+1}\\
\hat{w}&=(\hat{w}_m^{(j)})^T_{m=0,\dots,M;j=1,\dots,H}\in\R^{M+1\times H}\\
u&=(u(T,x_j,y_i)^T_{i=1,\dots,S;j=1,\dots,H}\in\R^{S\times H}\\
\end{align*}
Für jeden Knotenpunkt $x_j$ ist also ein lineares Gleichungssystem der Größe $S\times M+1$ zu lösen. Die obige Schreibweise erlaubt es in vielen Programmiersprachen die Systeme gleichzeitig zu lösen und bietet somit einen starken Laufzeitvorteil gegenüber dem Lösen von $H$ separaten Gleichungen. Um sicherzustellen, dass das System nicht unterbestimmt ist, muss gelten $S\ge M+1=\binom{N+P}{P}$. Ist $S\neq M+1$, so verwenden wir die Pseudoinverse von $A$. Später werden wir an verschiedenen Beispielen Konsequenzen dieser Bedingung genauer betrachten.
\begin{mathbem}
In Gleichung (\ref{eqn:gpc_approx_exp}) wurde gezeigt, dass sich der Erwartungswert an der Stelle $x_j$ approximieren lässt durch
\[\E[u(t,x_j,Y)]\approx \hat{w}_0^{(j)}\]
Da die Gleichungen gekoppelt sind, müssen aber trotzdem auch alle anderen $\hat{w}_m$ berechnet werden.
\end{mathbem}
\subsection{Wahl der Interpolationspunkte}
Wir wollen direkt am Anfang darauf hinweisen, dass im mehrdimensionalen für $N>1$ die Wahl der Knotenpunkte für die Interpolation ein nicht vollständig verstandenes Problem darstellt. Das Konzept der Lagrange-Interpolation, die im eindimensionalen für jede beliebige Knotenmenge möglich ist, lässt sich wie in \autocite{San07} gezeigt konzeptuell auf den mehrdimensionalen Fall erweitern. Allerdings benötigt sie die entsprechende Bedingung $\det(A)\neq 0$, welche besagt, dass die Interpolation durch die gegebenen Knotenpunkte eindeutig ist. Die Beziehung dieser Bedingung zur Wahl der Knotenpunkte stellt ein "'komplexes Problem"' dar.
\subsubsection*{Grundlagen der eindimensionalen Interpolation}
Der Satz von Cauchy ist der erste Orientierungspunkt für die Abhängigkeit der Interpolationsgüte zur Wahl der Knotenpunkte für den eindimensionalen Fall und einem kompakten Intervall.
\begin{maththeorem}[Cauchy]
Sei $f\in C^{S}[-1,1]$ eine $S$ mal stetig differenzierbare Funktion. Dann ist für jede Knotenmenge $\lbrace y_1,\dots,y_{S}\rbrace$ und $y\in[-1,1]$ der Interpolationsfehler gegeben durch
\[f(y)-\mathcal{Q}_{S}f=\frac{f^{(S)}(\xi)}{S!}\prod_{i=1}^{S}(y-y_i),\quad \xi\in [-1,1]\]
\end{maththeorem}

Da die Funktion gegeben ist, hat man keinen Einfluss auf die Ableitung $f^{(S)}$. Wegen
\[\norm{f(y)-\mathcal{Q}_{S}f}\le \frac{\norm{f^{(S)}}_\infty}{S!}\underbrace{\norm{\prod_{i=1}^{S}(y-y_i)}}_{=\norm{w(y)}}\]
ist es somit das Ziel, den Term $\norm{w(y)}$ abhängig von der Norm zu minimieren.
\begin{mathbem}[\chebyspace Interpolation]
\label{bem:cheby}
Die Nullstellen $y_i=\cos\left(\pi\frac{2i-1}{2S+2}\right)$, für $i=1,\dots,S+1$, der \chebyspace Polynome $T_{S+1}(y)=\cos((S+1)\arccos(y))$ minimieren den Ausdruck $\norm{w(y)}_\infty$ auf $[-1,1]$ und bieten auf $[-1,1]$ optimale Interpolationsgüte. Es gilt 
\[\norm{f(y)-\mathcal{Q}_{S+1}f}_\infty\le \frac{\norm{f^{(S+1)}}_\infty}{2^S(S+1)!}\]
\end{mathbem}
Die \chebyspace Polynome bilden eine Orthogonalbasis bezüglich $\langle \cdot,\cdot\rangle_{L_w^2[-1,1]}$ mit $w(y)=(1-y^2)^{-\onehalf}$. Eine Idee ist es nun, auch Nullstellen anderer orthogonalen Polynombasen als Knotenpunkte zu verwenden um ähnliche Approximationsergebnisse auf nicht-kompakten Intervallen zu erhalten.
\begin{maththeorem}
\label{th:interpol_and_proj}
Sei $\lbrace \Phi_m\in\Poly_m\mid m=0,\dots,M\rbrace\subset L_w^2(I)$ eine Orthonormalbasis von Polynomen und $(y_i,a_i)_{i=0,\dots,M}$ eine Quadraturformel mit Exaktheitsgrad $2M$ bezüglich $w$ und $y_i\neq y_j,i\neq j$, d.h. \[\int_I p(y)w(y)dy=\sum_{i=0}^M a_ip(y_i),\quad p\in\Poly_{2M}\]
Sei weiter das Interpolationspolynom  $\mathcal{Q}_Mf$ einer Funktion $f\colon I\to\R$ bezüglich den Interpolationspunkten $\lbrace y_i\mid i=0,\dots,M\rbrace$ beschrieben durch
\[\mathcal{Q}_Mf(y)=\sum_{m=0}^M\hat{w}_m\Phi_m(y)\]
Die Koeffizienten $\hat{w}_m$ werden in Analogie zu (\ref{eqn:interpol_compact}) berechnet durch das Lösen des linearen Gleichungssystems
\[A\hat{w}=\hat{f}\]
Dann ist \[\hat{w}_i=\sum_{j=0}^M\Phi_i(y_j)a_jf(y_j)\approx \int_I \Phi_i(y)f(y)w(y)dy,\quad i=0,\dots,M\]
\end{maththeorem}
\begin{proof}
Es ist $A_{ij}=\Phi_j(y_i)$. Da wir uns im eindimensionalen Fall befinden ($N$=1) ist die Vandermonde-artige Matrix $A$ invertierbar, da die $\Phi_i$ eine Basis bilden und die $y_i$ paarweise verschieden sind. Es gilt für $\widetilde{w}_i\coloneqq \sum_{j=0}^M\Phi_i(y_j)a_jf(y_j)$ und $\widetilde{w}\coloneqq (\widetilde{w}_i)^T$
\begin{align*}
\left(A\widetilde{w}\right)_i&=\sum_{j=0}^M\Phi_j(y_i)\widetilde{w}_j\\
&=\sum_{j=0}^M\Phi_j(y_i)\sum_{k=0}^M\Phi_j(y_k)a_kf(y_k),\quad \text{Interpol.:} f(y_k)=\sum_{\ell=0}^M\hat{w}_\ell\Phi_\ell(y_k)\\
&=\sum_{\ell=0}^M\hat{w}_\ell \sum_{j=0}^M\Phi_j(y_i)\sum_{k=0}^M\Phi_j(y_k)a_k\Phi_\ell(y_k)\\
&\stackrel{\text{deg}(\Phi_j\Phi_\ell)\le 2M}{=}\sum_{\ell=0}^M\hat{w}_\ell\sum_{j=0}^M\Phi_j(y_i)\underbrace{\int_I\Phi_j(y)\Phi_\ell(y)w(y)dy}_{=\delta_{j\ell}}\\
&=\sum_{\ell=0}^M\hat{w}_\ell\Phi_\ell(y_i)=f(y_i)=\hat{f}_i
\end{align*}
Also gilt $A\widetilde{w}=\hat{f}=A\hat{w}$ und da $A$ regulär ist somit $\widetilde{w}=\hat{w}$.
\end{proof}
\begin{mathbem}
Wir wollen nun dem technischen Satz \ref{th:interpol_and_proj} Leben einhauchen und seine Relevanz verdeutlichen.
\begin{itemize}
\item
Die vorausgesetzte Existenz einer Quadraturformel aus $M+1$ verschiedenen Quadraturpunkten und Exaktheitsgrad $2M$ ist durch die später vorgestellte Gauss-Quadratur erfüllt. Die Quadraturpunkte sind dabei die Nullstellen des $M+1$-ten Basispolynoms und daher paarweise verschieden. Die Quadraturgewichte lassen sich über die erste Komponente dessen Einheitsvektors bestimmen, der beim Golub-Welsch-Algorithmus auch die entsprechende Nullstelle des Polynoms berechnet.
\item
Wir sehen, dass sich im eindimensionalen bei bekannter Quadraturformel die Interpolationskoeffizienten $\hat{w}_m$ auch einzeln ohne Lösen eines linearen Gleichungssystems berechnen lassen. Diese Darstellung führt auf den später vorgestellten zweiten Ansatz zur stochastischen Collocation.
\item
Die Darstellung $\hat{w}_i=\sum_{j=0}^M\Phi_i(y_j)a_jf(y_j)$ kann als Approximation an die Koeffizienten $\int_I \Phi_i(y)f(y)w(y)dy$ der Bestapproximation $P_Mf$ in $L_w^2(I)$ aufgefasst werden. Mithilfe der Dreiecksungleichung lässt sich dann der Fehler der Interpolation beschreiben durch
\begin{align*}
&\norm{\mathcal{Q}_Mf-f}_{L_w^2}\le \underbrace{\norm{P_Mf-f}_{L_w^2}}_{\text{Fehler der Bestapproximation}}+\underbrace{\norm{\mathcal{Q}_Mf-P_Mf}_{L_w^2}}_{\text{Aliasing Fehler}}\\
&=\norm{\mathcal{Q}_Mf-P_Mf}_{L_w^2}+\norm{\sum_{m=0}^M\left(\sum_{j=0}^M\Phi_m(y_j)a_jf(y_j)\right)\Phi_m-\sum_{m=0}^M\left(\int_I\Phi_m(y)f(y)w(y)dy\right)\Phi_m}_{L_w^2}\\
&\stackrel{\text{Parseval}}{=}\norm{\mathcal{Q}_Mf-P_Mf}_{L_w^2}+\sqrt{\sum_{m=0}^M\left(\sum_{j=0}^M\Phi_m(y_j)a_jf(y_j)-\int_I\Phi_m(y)f(y)w(y)dy\right)^2}
\end{align*}
Schlussendlich lässt sich also der Fehler der Interpolation durch den Fehler der Bestapproximation in $L_w^2$ und einen Quadraturfehler bezüglich der Gewichtsfunktion $w$ beschreiben. Für den Fehler der Bestapproximation erwarten wir nach Xiu spektrale Konvergenz. Finden wir eine zur Gewichtsfunktion $w$ passende Quadraturformel, so hat lediglich die Glattheit von $f$ Einfluss auf den Quadraturfehler, da $\Phi_m$ als Polynom bei hinreichend hoher Ordnung exakt integriert wird.
\end{itemize}
\end{mathbem}
\begin{mathbsp}
Wir vergleichen anhand von \nameref{trial:1} für $d=1$ zwei mögliche Wahlen der Interpolationspunkte. Da $Y$ eine Gleichverteilung zugrunde liegt, verwenden wir die normalisierten Legendre-Polynome $L_0,\dots,L_M$. Als Interpolationspunkte $y_i$ wählen wir dann
\begin{enumerate}
\item die $M+1$ Nullstellen des Legendre-Polynoms vom Grad $M+1$ (genannt \textit{full\_tensor}).
\item die $M+1$ Nullstellen des \chebyspace Polynoms vom Grad $M+1$ (siehe Bemerkung \ref{bem:cheby}).
\end{enumerate}
In Abbildung \ref{fig:collocation_trial1} ist die Approximationsgüte der Interpolation dargestellt, links für $\tau=0.001$ und rechts für $\tau=0.0001$, gemessen durch die Fehler an den Erwartungswert und die Varianz. Man erkennt, dass die \chebyspace Interpolation langsamer konvergiert als die Legendre-Interpolation. Die Konvergenz stoppt bei einer gewissen Genauigkeit, die abhängig ist von $\tau$. Dies ist dadurch begründet, dass die Punkte $u(T,x_j,y_i)$ nicht exakt vorliegen sondern lediglich mithilfe eines Strang-Splittings mit Schrittweite $\tau$ approximiert werden. Dies betont die Wichtigkeit der Wahl eines kleinen $\tau$, allerdings bedeutet dies gleichzeitig eine entsprechende multiplikative Vergrößerung der Berechnungszeit und somit insbesondere für $N>1$ einen deutlichen Mehraufwand.
\begin{figure}[!htb]
\minipage{0.5\textwidth}
  \includegraphics[width=\linewidth]{Figures/collocation_mi_trial1_tau001.png}
\endminipage
\minipage{0.5\textwidth}
  \includegraphics[width=\linewidth]{Figures/collocation_mi_trial1_tau0001.png}
\endminipage
\caption{Vergleich von verschiedenen Interpolationspunkten und der Abhängigkeit der bestmöglichen Interpolation von der Genauigkeit der Interpolationspunkte anhand von \nameref{trial:1} zum Zeitpunkt $T=0.5$ und für $d=1$.}
\label{fig:collocation_trial1}
\end{figure}
\end{mathbsp}
\subsubsection*{Mehrdimensionale Interpolation durch volles Tensorprodukt}
Eine mögliche Wahl für $N>1$ die Menge der Interpolationspunkte $\Theta_Y^N$ zu einem gegebenem Zufallsvektor $Y$ der Dimension $N$ zu wählen, ist gegeben durch das volle Tensorprodukt
\[\Theta_Y^N=\Theta_{Y_1}\otimes \dots \otimes \Theta_{Y_N},\quad \Theta_{Y_i}\subset I_{Y_i}\]
Dann gilt für die Anzahl der Interpolationspunkte $S=S_1 \dots S_N$. Ist die Anzahl an Basispolynomen $M+1$ fest, so ist es schwierig die Bedingung $S=M+1$ exakt zu erfüllen. Wir fordern daher, dass $S_1=\dots=S_N$ und $S\ge M+1$ minimal. Dieser Ansatz wird in Abbildungen durch \textit{full\_tensor} gekennzeichnet.
\subsubsection*{Mehrdimensionale Interpolation durch zentralisiertes dünnes Gitter}
Eine weitere Option für $M+1=\binom{N+P}{P}$ ist mit $S_1=\dots=S_N$ die Wahl
\[\Theta_Y^N=\lbrace (y_{m_1},\dots,y_{m_N})\in \Theta_{Y_1}\otimes \dots \otimes \Theta_{Y_N} \mid |m|\le P \rbrace \]
Es wird somit jedem Multi-Index $m\in\N_0^N$ genau ein mehrdimensionaler Interpolationspunkt $y$ zugewiesen und es gilt $M+1=S$. Die Wahl der Nummerierung der eindimensionalen Interpolationspunkte ist dabei entscheidend. Für Legendre- oder Hermite-Polynome bietet sich eine betragsmäßig aufsteigende Sortierung der Punkte an, da diese symmetrisch um $0$ verteilt sind. Für Jacobi- und Laguerre-Polynome ist diese Symmetrie abhängig von den Parametern verschoben. Man beobachtet in Abbildung \ref{fig:poly_roots} dennoch die für Orthogonalpolynome geltende Eigenschaft, dass die Nullstellen eines Polynoms höheren Grades zwischen den Nullstellen des vorherigen Polynoms zu finden sind. Dies motiviert es die Nullstellen um den Median herum zu sortieren. Dieser Ansatz wird in Abbildungen durch \textit{centralized} gekennzeichnet.
\begin{figure}[!htb]
\minipage{0.5\textwidth}
  \includegraphics[width=\linewidth]{Figures/roots_legendre.png}
  \includegraphics[width=\linewidth]{Figures/roots_jacobi.png}
\endminipage
\minipage{0.5\textwidth}
  \includegraphics[width=\linewidth]{Figures/roots_hermite.png}
  \includegraphics[width=\linewidth]{Figures/roots_laguerre.png}  
\endminipage
\caption{Nullstellen verschiedener Orthogonalbasen von Polynomen mit steigendem Grad.}
\label{fig:poly_roots}
\end{figure}

Ein Beispiel sei für $N=2$ und $P=14$ gemacht, wo für Legendre- und Hermite-Polynome die Interpolationspunkte des zentralisierten Gitters in Abbildung \ref{fig:roots_centralized} dargestellt sind. Der Vorteil dieser Methode ist, dass durch die Eigenschaft $M+1=S$ die Matrix quadratisch ist und die Kondition kleiner ist als für ein überbestimmtes System. Wir werden später basierend auf Smolyaks Konstruktion einen effizienteren dünne Gitter Ansatz kennen lernen, der nicht nur für Interpolation verwendbar ist, sondern sich auch auf generelle Operatoren übertragen lässt.
\begin{figure}[!htb]
\centering
\includegraphics[width=0.8\linewidth]{Figures/roots_centralized.png}
\caption{Interpolationspunkte des centralized Ansatzes für $N=2$ und Legendre- und Hermite-Polynome.}
\label{fig:roots_centralized}
\end{figure}
\subsection{Vergleiche der Ansätze}
Wir reduzieren uns für $N=1$ auf die Nullstellen derjenigen Orthogonalpolynome, die gemäß des gPC zu der Verteilung von $Y$ gehören. Für das kompakte Interval $I=[-1,1]$ wäre es ebenso möglich die \chebyspace Interpolationspunkte zu verwenden. Diese bieten aber nach unseren Beobachtungen keinen Vorteil.\\
Für $N>1$ besteht die Möglichkeit das volle Tensorprodukt der jeweiligen eindimensionalen Interpolationspunkte zu den Komponenten von $Y$ zu wählen. Da dies jedoch im Allgemeinen mehr Punkte umfasst als die vorhandenen $M+1$ Polynome, ist das lineare Gleichungssystem überbestimmt und wir beobachten eine hohe Kondition der Matrix. Dies sorgt insbesondere dafür, dass die Approximation der Varianz, welche jede Komponente des Lösungsvektors benötigt, bereits für $P>3$ instabil ist.\\
Anhand der \nameref{trial:4} aus dem Appendix mit $N=4$ vergleichen wir die beiden Wahlen in Abbildung \ref{fig:collocation_trial4}. Wir sehen, dass sich der Erwartungswert mit beiden Ansätzen gleich gut approximieren lässt. Die Varianz ist durch das überbestimmte System beim full\_tensor Ansatz jedoch nicht berechenbar (für $P=9$ ergibt sich ein System der Größe $6^4\times \binom{4+9}{9} =1296\times 715$).
\begin{figure}[!htb]
\centering
\includegraphics[width=\linewidth]{Figures/collocation_mi_trial4_ft_centralized.png}
\caption{Approximation des Erwartungswerts und der Varianz von \nameref{trial:4} mithilfe von Collocation durch Interpolation und verschiedenen Ansätzen.}
\label{fig:collocation_trial4}
\end{figure}

\section{Diskrete Projektion}
Der zweite Ansatz zur stochastischen Collocation basiert auf der Bestapproximation in $L_{\rho_Y}^2$.
\begin{mathdef}[Diskrete Projektion]
Sei $Y$ ein Zufallsvektor und $\Phi_m(Y)$ für $m=0,\dots M$ die zugehörigen orthonormalen gPC Basisfunktionen. Dann ist die Bestapproximation der sKGG zu einem Zeitpunkt $T>0$ und für eine Stelle $x_j$ definiert als
\[P_Mu(T,x_j,Y)=\sum_{m=0}^M\hat{u}_m^{(j)}\Phi_m(Y)\]
mit Koeffizienten
\[\hat{u}_m^{(j)}=\E[u(T,x_j,Y)\Phi_m(Y)]=\int_{I_Y} u(T,x_j,y)\Phi_m(y)\rho_Y(y)dy\]
Ist nun $(y_i,a_i)_{i=0,\dots,Q}$ eine Quadraturformel für das Gewicht $\rho_Y$, so ist die diskrete Projektion definiert als
\[w_{M,Q}(T,x_j,Y)=\sum_{m=0}^M\hat{w}_m^{(j)}\Phi_m(Y)\] wobei für die Koeffizienten gilt
\[\hat{w}_m^{(j)}=\sum_{i=0}^Qu(T,x_j,y_i)\Phi_m(y_i)a_i\approx \int_{I_Y} u(T,x_j,y)\Phi_m(y)\rho_Y(y)dy\]
\end{mathdef}
Satz \ref{th:interpol_and_proj} zeigt, dass für $Q=M$ und eine Quadraturformel mit paarweise verschiedenen Quadraturpunkten und Exaktheitsgrad $2M$ die beiden Ansätze im eindimensionalen äquivalent sind. Aus der Bemerkung nach Satz \ref{th:interpol_and_proj} folgt auch, dass sich der Fehler der diskreten Projektion mithilfe der Dreiecksungleichung durch den Fehler der Bestapproximation und einen Quadraturfehler, den Aliasing Fehler, abschätzen lässt.

\subsection{Gauss-Quadratur im eindimensionalen}
Wir wollen nun eine möglichst exakte Quadraturformel konstruieren, welche auf natürliche Weise das jeweilige Gewicht $\rho$ des Integrals berücksichtigt.
\begin{maththeorem}[Gauss-Quadratur]
Sei $Q_0,\dots,Q_{n+1}\in L_\rho^2(I)$ eine Orthogonalbasis von Polynomen bezüglich des Gewichts $\rho$. Es sei $\text{deg}(Q_j)=j$ für $j=0,\dots,n+1$. Seien $y_0,\dots,y_n$ die Nullstellen des Polynoms $Q_{n+1}$. Dann ist die Quadraturformel $(y_i,a_i)$ mit 
\[\int_If(y)\rho(y)dy\approx \sum_{i=0}^na_if(y_i)\]
und 
\[a_i=\int_I \rho(y)\underbrace{\prod_{j\neq i} \frac{y-y_j}{y_i-y_j}}_{=\ell_i(y)}dy\]
exakt für alle Polynome vom Grad höchstens $2n+1$.
\end{maththeorem} 
\begin{proof}[Beweisskizze]
Sei $f$ ein Polynom vom Grad höchstens $2n+1$ und 
\[f(y)=Q_{n+1}(y)\underbrace{p(y)}_{deg \le n}+\underbrace{r(y)}_{deg \le n}\]
eine durch Polynomdivision erhaltene Zerlegung von $f$. Da die $y_i$ Nullstellen von $Q_{n+1}$ sind, gilt
\[f(y_i)=r(y_i),\quad i=0,\dots n\]
Dann ist
\[\int_If(y)\rho(y)dy=\underbrace{\int_IQ_{n+1}(y)p(y)\rho(y)dy}_{=0}+\int_Ir(y)\rho(y)dy\]
da $Q_{n+1}$ orthogonal zum Raum aller Polynome vom Grad höchstens $n$ ist.\\
Die interpolatorischen Quadraturformeln (wie beispielsweise die Newton-Cotes-Formeln) mit den Gewichten $a_i$, die über das gewichtete Integral der Lagrange-Polynome $\ell_i$ definiert werden, sind exakt für Polynome vom Grad höchstens $n$. Somit gilt
\[\int_If(y)\rho(y)dy=\int_Ir(y)\rho(y)dy=\sum_{i=0}^{n}a_ir(y_i)=\sum_{i=0}^{n}a_if(y_i)\]
\end{proof}
\begin{mathbem}
Wir nennen an dieser Stelle noch einige praktische Hinweise zur Gauss-Quadratur.
\begin{itemize}
\item Die Gauss-Quadraturformel zur entsprechenden gPC Basis erfüllt die Voraussetzungen an die Quadraturformel von Satz \ref{th:interpol_and_proj}.
\item Die Nullstellen $y_i$ lassen sich als die Eigenwerte einer symmetrischen Tridiagonalmatrix mithilfe des Golub-Welsch-Algorithmus berechnen.
\item Die Gewichte $a_i$ lassen sich ebenfalls mithilfe des Golub-Welsch-Algorithmus berechnen. Ist $y_i$ ein Eigenwert der symmetrischen Tridiagonalmatrix $J$ mit normalisiertem Eigenvektor $z$, d.h. $Jz=y_iz$ und $\norm{z}=1$, so gilt
\[a_i=(z_1)^2\]
Das Gewicht ist somit das Quadrat des ersten Eintrags des Eigenvektors, siehe \autocite{GolubWelsch}.
\end{itemize}
\end{mathbem}
\todo[inline]{Nenne Vorteile der Interpolation: Keine Gewichte benötigt, Nodes "beliebig" und flexibler, evtl. somit besser für sparse grids da man auch glenshaw curtis nehmen kann, info über Stabilität durch cond der Matrix. Nachteil: gekoppelt, LGS muss gelöst werden (aufwändig), häufig unter/überbestimmt}
\todo[inline]{Nachteil der DP: Konvergenz abhängig von zweitem Freiheitsgrad Q anstelle von nur M, aber für erwartungswert praktisch da dort eh nur eins nötig ist und somit "beliebig genau" mit Q steuerbar}
\subsection{Mehrdimensionale Quadratur}
Um den Ansatz der diskreten Projektion für einen mehrdimensionalen Zufallsvektor mit $N>1$ durchzuführen, benötigen wir eine mehrdimensionale Quadratur. Im Gegensatz zur mehrdimensionalen Interpolation liegt die Schwierigkeit nicht nur darin eine Menge von Collocationspunkten zu finden, sondern auch die zugehörige Menge der Quadraturgewichte zu bestimmen.\\
Die numerischen Möglichkeiten der mehrdimensionalen Integration (engl. \emph{cubature}) umfassen unter anderem adaptive Verfahren und Monte-Carlo-Methoden. Da diese auf einer großen Menge an Funktionsauswertungen basieren und in unserem Fall eine Funktionsauswertung das aufwändige Lösen einer Instanz der sKGG bedeutet, benötigen wir somit angepasste Integrationsverfahren.
\subsubsection{Tensorkonstruktion}
Sei $Y$ ein Zufallsvektor und $A_{Y_i}$ ein eindimensionaler Quadraturoperator bezüglich des Gewichts $\rho_{Y_i}$, beispielsweise die Gauss-Quadratur. Dann erhalten wir mithilfe einer Tensorkonstruktion die mehrdimensionale Quadratur
\[A_Y=A_{Y_1}\otimes\dots\otimes A_{Y_N}\]
Für die Menge an Quadraturpunkten gilt wie schon bei der Interpolation
\[\Theta_Y=\Theta_{Y_1}\otimes\dots\otimes \Theta_{Y_N}\]
Die Anzahl der Quadraturpunkte $Q=Q_1\dots Q_N$ wächst daher wiederum sehr stark und dies macht den Ansatz bereits für geringe Dimensionen $N$ sehr rechenaufwändig. Andererseits sehen wir sofort, dass sich die polynomiale Exaktheit der Gauss-Quadraturen direkt auf den mehrdimensionalen Fall übertragen lässt. Die Quadratur $A_Y$ ist exakt für alle Polynome in
\[\Poly_{2Q_1-1}(Y_1)\otimes\dots\otimes\Poly_{2Q_N-1}(Y_N)\]
\subsubsection{Smoylaks dünne Gitter}
\todo[inline]{Smolyak Konstruktion erklären für Punkte Gitter wie bei Interpol benötigt aber auch für allgemeine Operatoren, welche Quelle?}

\todo[inline]{Quadratur (Xiu Kap7) mit Glenshaw-Curtis; smolyak sparse grids (fully nested failed, weakly nested and non nested; discrete projection}
